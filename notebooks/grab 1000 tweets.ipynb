{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load tweets older then two weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(db_path, days):\n",
    "    time_delta = date.today() - timedelta(days=days)\n",
    "    cnx = sqlite3.connect(db_path)\n",
    "    query = f\"SELECT id,user, full_text, created_at, lang, quoted_status, in_reply_to_status_id FROM tweets WHERE created_at < '{str(time_delta)}'\"\n",
    "    #TODO add restrain, to remove tweets I liked, but for that I need to setup another cron job too.\n",
    "    df = pd.read_sql_query(\n",
    "        query,\n",
    "        cnx,\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = load_tweets(\"../home.db\", days=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_tweets.columns:\n",
    "    if df_tweets[col].dtype == \"float64\":\n",
    "        df_tweets[col] = df_tweets[col].fillna(0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(tweet):\n",
    "    return re.findall(r\"http\\S+\", tweet)\n",
    "\n",
    "\n",
    "def clean_links(tweet):\n",
    "    tweet = re.sub(r\"bit.ly/\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"t.co/\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"buff.ly/\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"twitter.com/\\S+\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "def expand_link(url):\n",
    "    if url in [\"https://\", \"http://\"]:\n",
    "        return url\n",
    "    else:\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            resp = session.head(url, allow_redirects=True)\n",
    "            long_url = resp.url\n",
    "            return long_url\n",
    "        except:\n",
    "            return url\n",
    "\n",
    "def get_domain(url):\n",
    "    domain = urlparse(url).netloc\n",
    "    dot_split = domain.split(\".\")\n",
    "    if len(dot_split) > 2:\n",
    "        return \".\".join(dot_split[1:])\n",
    "    else:\n",
    "        return domain\n",
    "\n",
    "\n",
    "def remove_empty_str(l):\n",
    "    for i in l:\n",
    "        if len(i) == 0:\n",
    "            l.remove(i)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searching for not-expanded news urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfz = df_tweets[df_tweets.id == 1339612008182919168]\n",
    "dfz[\"urls\"] = dfz[\"full_text\"].apply(find_url)\n",
    "dfz[\"urls\"] = dfz.urls.apply(lambda x: [clean_links(d) for d in x])\n",
    "dfz[\"domains\"] = dfz.urls.apply(lambda x: [get_domain(d) for d in x])\n",
    "dfz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz = df_tweets.copy()\n",
    "dfz[\"urls\"] = dfz[\"full_text\"].apply(find_url)\n",
    "dfz[\"urls\"] = dfz.urls.apply(lambda x: [clean_links(d) for d in x])\n",
    "dfz[\"domains\"] = dfz.urls.apply(lambda x: [get_domain(d) for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz[\"urls_exp\"] = dfz.urls.apply(lambda x: [expand_link(d) for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz[\"domains_exp\"] = dfz.urls_exp.apply(lambda x: [get_domain(d) for d in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def find_news(df, news_domains_list):\n",
    "\n",
    "    df[\"urls\"] = df[\"full_text\"].apply(find_url)\n",
    "    df[\"urls\"] = df.urls.apply(lambda x: [clean_links(d) for d in x])\n",
    "    df[\"domains\"] = df.urls.apply(lambda x: [get_domain(d)\n",
    "    543  for d in x])\n",
    "    df[\"domains\"] = df.domains.apply(remove_empty_str)\n",
    "    df.drop([\"urls\"], axis=1, inplace=True)\n",
    "\n",
    "    new_columns_list = []\n",
    "    max_nr_dom = df.domains.str.len().max()\n",
    "    for i in range(max_nr_dom):\n",
    "        new_columns_list.append(f\"domain{i+1}\")\n",
    "    df[new_columns_list] = pd.DataFrame(df.domains.tolist())\n",
    "\n",
    "    for col in new_columns_list:\n",
    "        df[col] = df[col].isin(news_domains_list)\n",
    "\n",
    "    df.drop([\"domains\"], axis=1, inplace=True)\n",
    "\n",
    "    df[\"contains_news\"] = df[new_columns_list].sum(axis=1)\n",
    "    df[\"contains_news\"] = df.contains_news.apply(lambda x: x if x == 0 else 1)\n",
    "    df.drop(new_columns_list, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_contains(df, column_name, word_list):\n",
    "    for string in word_list:\n",
    "        df[\"lower\"] = df[\"full_text\"].str.lower()\n",
    "        df = df[df[\"lower\"].str.contains(string)]\n",
    "        df.drop([\"lower\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_domains.txt\", \"r\") as f:\n",
    "    news_domains = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_tweets = find_news(df_tweets, news_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_in_qt_rt(df):\n",
    "    #TODO refractor this\n",
    "    df[\"all_news\"] = df[\"contains_news\"].copy()\n",
    "\n",
    "    df_qt = df[[\"id\", \"contains_news\"]].copy()\n",
    "    df_qt.columns = [\"quoted_status\", \"qt_news\"]\n",
    "    df = df.merge(df_qt, on=\"quoted_status\", how=\"left\")\n",
    "    df[\"qt_news\"] = df[\"qt_news\"].fillna(0).astype(np.int64)\n",
    "    df[\"all_news\"] = df[\"qt_news\"].astype(np.int64) + df[\"contains_news\"].astype(np.int64)\n",
    "\n",
    "    df_qt = df[[\"id\", \"contains_news\"]].copy()\n",
    "    df_qt.columns = [\"in_reply_to_status_id\", \"rt_news\"]\n",
    "    df = df.merge(df_qt, on=\"in_reply_to_status_id\", how=\"left\")\n",
    "    df[\"rt_news\"] = df[\"rt_news\"].fillna(0).astype(np.int64)\n",
    "    df[\"all_news\"] = df[\"rt_news\"].astype(np.int64) + df[\"all_news\"].astype(np.int64)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"in_reply_to_status_id\"]:\n",
    "    df[col] = df[col].fillna(0).astype(np.int64)\n",
    "\n",
    "df = news_in_qt_rt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets.id == 1335980504735072256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 1335967310843060230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't have QT in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1335980504735072256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"in_reply_to_status_id\"] = df[\"in_reply_to_status_id\"].fillna(0).astype(np.int64)\n",
    "\n",
    "df_qt = df[[\"id\", \"contains_news\"]].copy()\n",
    "df_qt.columns = [\"in_reply_to_status_id\", \"rt_news\"]\n",
    "df = df.merge(df_qt, on=\"in_reply_to_status_id\", how=\"left\")\n",
    "df[\"rt_news\"] = df[\"rt_news\"].fillna(0).astype(np.int64)\n",
    "df[\"all_news\"] = df[\"rt_news\"].astype(np.int64) + df[\"all_news\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_qt, on=\"in_reply_to_status_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_tweets = news_in_qt_rt(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets.id == 1197689942396174336]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seen_tweets = pd.read_csv(\"seen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_tweets.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets[\"id\"].isin(seen_tweets[\"tweet_id\"].tolist())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(df_tweets.shape)\n",
    "df_tweets = df_tweets[~df_tweets[\"id\"].isin(seen_tweets[\"tweet_id\"].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter out non english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[df_tweets[\"lang\"] == \"en\"]\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suffle them and filter out news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[df_tweets[\"contains_news\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_contains(df, column_name, str_list, lower=True):\n",
    "    for string in str_list:\n",
    "        if lower:\n",
    "            df[\"filter\"] = df[column_name].str.lower().copy()\n",
    "        if not lower:\n",
    "            df[\"filter\"] = df[column_name].copy()\n",
    "        df = df_tweets[~df[\"filter\"].str.contains(string)]\n",
    "        df = df.drop([\"filter\"], axis=1).copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "🍿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_contains(df, column_name, str_list, lower=True):\n",
    "    for string in str_list:\n",
    "        if lower:\n",
    "            df[\"filter\"] = df[column_name].str.lower().copy()\n",
    "        if not lower:\n",
    "            df[\"filter\"] = df[column_name].copy()\n",
    "        df = df_tweets[df[\"filter\"].str.contains(string)]\n",
    "        df = df.drop([\"filter\"], axis=1).copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_show = [\"🍿\"]\n",
    "show_contains(df_tweets, column_name=\"full_text\", str_list = str_to_show, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_drop = [\"breaking:\"]\n",
    "df_tweets = drop_contains(df_tweets, column_name=\"full_text\", str_list = str_to_drop)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_drop = [\"GOP\"]\n",
    "df_tweets = drop_contains(df_tweets, column_name=\"full_text\", str_list = str_to_drop, lower=False)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"breaking:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"lower\"] = df_tweets[\"full_text\"].str.lower().copy()\n",
    "df_tweets = df_tweets[~df_tweets[\"lower\"].str.contains(string)]\n",
    "df_tweets.drop([\"lower\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"full_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_custom_news_feed = (\n",
    "    df_tweets[df_tweets[\"contains_news\"] == 0]\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)[:1000]\n",
    ")\n",
    "to_custom_news_feed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_custom_news_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_custom_news_feed[[\"id\", \"user\"]].to_csv(\"batch_to_add.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "642.15px",
    "left": "1714.93px",
    "right": "20px",
    "top": "120px",
    "width": "409.067px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
